{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I have removed guided comments and docstrings as they are of no use to seniors and it will give u a clean experience when reading the file. Tho if u want the one which had it , then u can see it [here](https://colab.research.google.com/drive/1i5yCdsgjtUQNSke-FnkUcyLEVHQBYgi3?usp=sharing).\n",
        "\n",
        "The answers.txt is also available in the repo but i have also included answers here just after the quest as a comment for ease of access.\n",
        "\n",
        "To view the current repo in collab , click [here](https://colab.research.google.com/drive/1pUW7cygkBlqzWHeFXqf7-3CrEAFK8eTk?usp=sharing)."
      ],
      "metadata": {
        "id": "GywIfv60lWKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-WS4_onGMRuX"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "# folder = \"./final/\" # folder containing the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ76BR75ZjKR",
        "outputId": "a5837f71-354f-45c7-ac96-bb5efa070dd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip = r'/content/drive/MyDrive/Storage abuse/Copy of fashion-minst.zip' #Please change the location acc to ur file\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as z:\n",
        "    z.extractall('/content/mnistdataset')\n",
        "\n",
        "folder = '/content/mnistdataset/final/'"
      ],
      "metadata": {
        "id": "3M0zCVreNSEz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = './mnistdataset/final/9/11.png'\n",
        "x = plt.imread(x)\n",
        "img = plt.imshow(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Nm7cXs5OV7fH",
        "outputId": "5c9c48b7-fdf9-491f-9ab9-f929054c08d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"0100111001100101011101100110010101110010001000000110011101101111011011100110111001100001001000000110011101101001011101100110010100100000011110010110111101110101001000000111010101110000\"\n",
        "np.random.seed(abs(hash(seed))%2**32)"
      ],
      "metadata": {
        "id": "IcKCiLpZlXlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jiO9g5ssMRuZ",
        "outputId": "902b9cbb-6178-4f04-b684-a164cb324895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4 class\n",
            "Loaded 5 class\n",
            "Loaded 2 class\n",
            "Loaded 6 class\n",
            "Loaded 3 class\n",
            "Loaded 9 class\n",
            "Loaded 7 class\n",
            "Loaded 0 class\n",
            "Loaded 8 class\n",
            "Loaded 1 class\n",
            "Before reshaping\n",
            "(60000, 28, 28, 4) (60000, 10)\n",
            "After reshaping\n",
            "(60000, 784) (60000, 10)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_data(X, y):\n",
        "    for f in os.listdir(folder):\n",
        "        for file in os.listdir(f\"{folder}/{f}\"):\n",
        "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
        "            X.append(img)\n",
        "\n",
        "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
        "\n",
        "            label = [0] * 10\n",
        "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
        "\n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"Loaded {f} class\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "# [Q2] Why convert to numpy array?\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Before reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "# print(X[0], y[0])\n",
        "\n",
        "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
        "print(\"After reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "# print(X[0], y[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "arf7mjPCMRub"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs, gamma = 1, step = 30 , beta = 0):\n",
        "\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "\n",
        "        # for scheduler\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.step = step\n",
        "\n",
        "        #for momentum\n",
        "        self.beta = beta\n",
        "\n",
        "\n",
        "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
        "        self.bih = np.zeros((hidden_neurons, 1))\n",
        "\n",
        "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
        "        self.bho = np.zeros((output_neurons, 1))\n",
        "\n",
        "        #Introduced velocity\n",
        "        self.v_wih = np.zeros_like(self.wih)\n",
        "        self.v_who = np.zeros_like(self.who)\n",
        "        self.v_bih = np.zeros_like(self.bih)\n",
        "        self.v_bho = np.zeros_like(self.bho)\n",
        "\n",
        "    def relu(self, z):\n",
        "        return z * (z > 0)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        return 1 * (z > 0)\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    def softmax_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def mean_squared_error(self, y, y_hat):\n",
        "        return np.mean((y - y_hat) ** 2, axis=0)\n",
        "\n",
        "    def cross_entropy_loss(self, y, y_hat):\n",
        "\n",
        "        loss = np.sum(-y*np.log(y_hat), axis = 0 , keepdims=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def mean_squared_error_derivative(self, y, y_hat):\n",
        "        #return y_hat - y        --------------------------> error ----> it should be 2*(y_hat-y)\n",
        "\n",
        "        return 2*(y_hat - y)\n",
        "\n",
        "    def cross_entropy_derivative(self, y, y_hat):\n",
        "\n",
        "      return (y_hat-y)/self.softmax_derivative(y_hat)  #On solving, CEL_derivative*softmax_derivative = yhat-y\n",
        "                                                       # so CEL_derivative = (yhat-y) / softmax_derivative\n",
        "\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs)\n",
        "\n",
        "        x0 = inputs\n",
        "        z1 = np.dot(self.wih, x0) + self.bih\n",
        "        a1 = self.relu(z1)\n",
        "\n",
        "        x1 = a1\n",
        "        z2 = np.dot(self.who, x1) + self.bho\n",
        "        a2 = self.softmax(z2)\n",
        "        return a2\n",
        "\n",
        "    def forward_modified(self, input_list):\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs)\n",
        "        x0 = inputs\n",
        "        z1 = np.dot(self.wih, x0) + self.bih\n",
        "        a1 = self.relu(z1)\n",
        "\n",
        "        x1 = a1\n",
        "        z2 = np.dot(self.who, x1) + self.bho\n",
        "        a2 = self.softmax(z2)\n",
        "\n",
        "        return {'yhat': a2,'final_inputs' : z2,\"hidden_outputs\": a1, \"hidden_inputs\":z1, 'inputs' : inputs}\n",
        "\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "\n",
        "        # inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
        "        # inputs = inputs - np.mean(inputs)\n",
        "\n",
        "        values = self.forward_modified(inputs_list)\n",
        "\n",
        "        tj = np.array(targets_list, ndmin=2).T                        #y\n",
        "\n",
        "        # hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        # hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "        hidden_inputs = values['hidden_inputs']\n",
        "        hidden_outputs = values['hidden_outputs']\n",
        "\n",
        "\n",
        "\n",
        "        # final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        # yj = self.softmax(final_inputs)                                    #yhat\n",
        "\n",
        "        final_inputs = values['final_inputs']\n",
        "        yj = values[\"yhat\"]\n",
        "\n",
        "        # loss = self.mean_squared_error(tj, yj) # Convert this to cross entropy loss\n",
        "        loss = self.cross_entropy_loss(tj, yj)\n",
        "\n",
        "        # Change this to cross entropy loss\n",
        "        # dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
        "\n",
        "        dE_dzo = self.cross_entropy_derivative(tj, yj) * self.softmax_derivative(yj)\n",
        "\n",
        "\n",
        "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1]\n",
        "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        inputs = values['inputs']\n",
        "        # Hidden Layer\n",
        "        dE_dah = np.dot(self.who.T, dE_dzo)\n",
        "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
        "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
        "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        # self.who -= self.lr * dE_dwho   #Commenting these to introduce momentum\n",
        "        # self.bho -= self.lr * dE_dbho\n",
        "        # self.wih -= self.lr * dE_dwih\n",
        "        # self.bih -= self.lr * dE_dbih\n",
        "\n",
        "        self.v_who = self.beta * self.v_who + (1 - self.beta) * dE_dwho\n",
        "        self.v_bho = self.beta * self.v_bho + (1 - self.beta) * dE_dbho\n",
        "        self.v_wih = self.beta * self.v_wih + (1 - self.beta) * dE_dwih\n",
        "        self.v_bih = self.beta * self.v_bih + (1 - self.beta) * dE_dbih\n",
        "\n",
        "        self.who -= self.lr * self.v_who\n",
        "        self.bho -= self.lr * self.v_bho\n",
        "        self.wih -= self.lr * self.v_wih\n",
        "        self.bih -= self.lr * self.v_bih\n",
        "\n",
        "\n",
        "\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            #-----------------    LOSS     ---------------#\n",
        "\n",
        "            loss = self.backprop(inputs_list, targets_list)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "            val_pred = self.forward(validation_data)\n",
        "            val_actual = validation_labels.T\n",
        "            vloss = np.mean(self.cross_entropy_loss(val_actual, val_pred))\n",
        "            val_loss.append(vloss)\n",
        "\n",
        "            #-----------------   ACCURACY  ----------------#\n",
        "\n",
        "            #train acc\n",
        "            tr_pred = self.forward(inputs_list).T   # tr_pred is y_pred for training\n",
        "            tr_actual = targets_list                # tr_actual is y_test for training data\n",
        "\n",
        "            tr_pred = np.argmax(tr_pred, axis = 1)\n",
        "            tr_actual = np.argmax(tr_actual, axis = 1)\n",
        "\n",
        "            train_acc = np.mean(tr_pred == tr_actual)*100\n",
        "\n",
        "            #val acc\n",
        "            val_pred = self.forward(validation_data).T\n",
        "            val_actual = validation_labels\n",
        "\n",
        "            val_pred = np.argmax(val_pred, axis = 1)\n",
        "            val_actual = np.argmax(val_actual, axis = 1)\n",
        "\n",
        "            val_acc = np.mean(val_pred == val_actual)*100\n",
        "\n",
        "\n",
        "            print('-'*100)\n",
        "            print(f\"| Epoch: {epoch:.3f} | Loss: {loss:.5f} | Val Loss: {vloss:.5f} | Train acc: {train_acc:.2f}% | Val_acc : {val_acc:.2f}% |\")\n",
        "            # if epoch%step == 0 :\n",
        "            #    self.lr = self.lr*self.gamma\n",
        "\n",
        "        # return train_loss[1:], val_loss[:-1]\n",
        "        return train_loss, val_loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QV1Df3v0MRud",
        "outputId": "4d504538-2d20-4570-c5b9-8a0eacea20d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 0.000 | Loss: 2.39926 | Val Loss: 2.10631 | Train acc: 29.78% | Val_acc : 28.95% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 1.000 | Loss: 2.10285 | Val Loss: 1.90423 | Train acc: 46.11% | Val_acc : 45.57% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 2.000 | Loss: 1.90200 | Val Loss: 1.74613 | Train acc: 54.54% | Val_acc : 54.37% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 3.000 | Loss: 1.74480 | Val Loss: 1.61654 | Train acc: 58.71% | Val_acc : 58.81% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 4.000 | Loss: 1.61565 | Val Loss: 1.50798 | Train acc: 61.15% | Val_acc : 61.19% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 5.000 | Loss: 1.50714 | Val Loss: 1.41630 | Train acc: 62.84% | Val_acc : 62.98% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 6.000 | Loss: 1.41525 | Val Loss: 1.33854 | Train acc: 64.25% | Val_acc : 64.51% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 7.000 | Loss: 1.33714 | Val Loss: 1.27232 | Train acc: 65.45% | Val_acc : 65.69% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 8.000 | Loss: 1.27050 | Val Loss: 1.21566 | Train acc: 66.65% | Val_acc : 66.67% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 9.000 | Loss: 1.21340 | Val Loss: 1.16683 | Train acc: 67.64% | Val_acc : 67.71% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 10.000 | Loss: 1.16414 | Val Loss: 1.12448 | Train acc: 68.49% | Val_acc : 68.50% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 11.000 | Loss: 1.12139 | Val Loss: 1.08745 | Train acc: 69.17% | Val_acc : 69.14% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 12.000 | Loss: 1.08398 | Val Loss: 1.05484 | Train acc: 69.80% | Val_acc : 69.86% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 13.000 | Loss: 1.05103 | Val Loss: 1.02590 | Train acc: 70.29% | Val_acc : 70.50% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 14.000 | Loss: 1.02179 | Val Loss: 1.00005 | Train acc: 70.80% | Val_acc : 70.98% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 15.000 | Loss: 0.99564 | Val Loss: 0.97679 | Train acc: 71.27% | Val_acc : 71.24% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 16.000 | Loss: 0.97210 | Val Loss: 0.95576 | Train acc: 71.63% | Val_acc : 71.78% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 17.000 | Loss: 0.95081 | Val Loss: 0.93664 | Train acc: 71.95% | Val_acc : 72.04% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 18.000 | Loss: 0.93144 | Val Loss: 0.91915 | Train acc: 72.30% | Val_acc : 72.32% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 19.000 | Loss: 0.91372 | Val Loss: 0.90310 | Train acc: 72.58% | Val_acc : 72.54% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 20.000 | Loss: 0.89745 | Val Loss: 0.88830 | Train acc: 72.82% | Val_acc : 72.76% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 21.000 | Loss: 0.88243 | Val Loss: 0.87460 | Train acc: 73.07% | Val_acc : 73.03% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 22.000 | Loss: 0.86853 | Val Loss: 0.86187 | Train acc: 73.36% | Val_acc : 73.17% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 23.000 | Loss: 0.85561 | Val Loss: 0.85000 | Train acc: 73.60% | Val_acc : 73.43% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 24.000 | Loss: 0.84357 | Val Loss: 0.83891 | Train acc: 73.80% | Val_acc : 73.67% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 25.000 | Loss: 0.83231 | Val Loss: 0.82850 | Train acc: 74.01% | Val_acc : 73.92% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 26.000 | Loss: 0.82174 | Val Loss: 0.81872 | Train acc: 74.22% | Val_acc : 73.99% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 27.000 | Loss: 0.81181 | Val Loss: 0.80950 | Train acc: 74.40% | Val_acc : 74.20% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 28.000 | Loss: 0.80245 | Val Loss: 0.80080 | Train acc: 74.60% | Val_acc : 74.38% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 29.000 | Loss: 0.79361 | Val Loss: 0.79256 | Train acc: 74.80% | Val_acc : 74.53% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 30.000 | Loss: 0.78526 | Val Loss: 0.78476 | Train acc: 74.99% | Val_acc : 74.68% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 31.000 | Loss: 0.77733 | Val Loss: 0.77734 | Train acc: 75.17% | Val_acc : 74.90% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 32.000 | Loss: 0.76980 | Val Loss: 0.77029 | Train acc: 75.36% | Val_acc : 75.06% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 33.000 | Loss: 0.76264 | Val Loss: 0.76356 | Train acc: 75.51% | Val_acc : 75.22% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 34.000 | Loss: 0.75582 | Val Loss: 0.75715 | Train acc: 75.73% | Val_acc : 75.33% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 35.000 | Loss: 0.74931 | Val Loss: 0.75101 | Train acc: 75.88% | Val_acc : 75.50% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 36.000 | Loss: 0.74309 | Val Loss: 0.74514 | Train acc: 76.05% | Val_acc : 75.66% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 37.000 | Loss: 0.73713 | Val Loss: 0.73952 | Train acc: 76.15% | Val_acc : 75.75% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 38.000 | Loss: 0.73143 | Val Loss: 0.73412 | Train acc: 76.29% | Val_acc : 75.87% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 39.000 | Loss: 0.72596 | Val Loss: 0.72894 | Train acc: 76.38% | Val_acc : 76.00% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 40.000 | Loss: 0.72071 | Val Loss: 0.72397 | Train acc: 76.49% | Val_acc : 76.13% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 41.000 | Loss: 0.71566 | Val Loss: 0.71918 | Train acc: 76.60% | Val_acc : 76.20% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 42.000 | Loss: 0.71081 | Val Loss: 0.71457 | Train acc: 76.72% | Val_acc : 76.28% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 43.000 | Loss: 0.70614 | Val Loss: 0.71012 | Train acc: 76.85% | Val_acc : 76.41% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 44.000 | Loss: 0.70163 | Val Loss: 0.70583 | Train acc: 76.96% | Val_acc : 76.57% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 45.000 | Loss: 0.69728 | Val Loss: 0.70168 | Train acc: 77.12% | Val_acc : 76.70% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 46.000 | Loss: 0.69308 | Val Loss: 0.69767 | Train acc: 77.22% | Val_acc : 76.84% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 47.000 | Loss: 0.68901 | Val Loss: 0.69379 | Train acc: 77.35% | Val_acc : 77.00% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 48.000 | Loss: 0.68509 | Val Loss: 0.69003 | Train acc: 77.46% | Val_acc : 77.08% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 49.000 | Loss: 0.68128 | Val Loss: 0.68638 | Train acc: 77.56% | Val_acc : 77.24% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 50.000 | Loss: 0.67759 | Val Loss: 0.68285 | Train acc: 77.65% | Val_acc : 77.33% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 51.000 | Loss: 0.67402 | Val Loss: 0.67942 | Train acc: 77.74% | Val_acc : 77.40% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 52.000 | Loss: 0.67055 | Val Loss: 0.67609 | Train acc: 77.80% | Val_acc : 77.49% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 53.000 | Loss: 0.66717 | Val Loss: 0.67285 | Train acc: 77.87% | Val_acc : 77.57% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 54.000 | Loss: 0.66390 | Val Loss: 0.66971 | Train acc: 77.99% | Val_acc : 77.58% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 55.000 | Loss: 0.66071 | Val Loss: 0.66664 | Train acc: 78.06% | Val_acc : 77.60% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 56.000 | Loss: 0.65761 | Val Loss: 0.66366 | Train acc: 78.18% | Val_acc : 77.67% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 57.000 | Loss: 0.65460 | Val Loss: 0.66075 | Train acc: 78.26% | Val_acc : 77.78% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 58.000 | Loss: 0.65166 | Val Loss: 0.65792 | Train acc: 78.34% | Val_acc : 77.90% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 59.000 | Loss: 0.64879 | Val Loss: 0.65516 | Train acc: 78.41% | Val_acc : 77.95% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 60.000 | Loss: 0.64600 | Val Loss: 0.65247 | Train acc: 78.48% | Val_acc : 78.06% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 61.000 | Loss: 0.64327 | Val Loss: 0.64984 | Train acc: 78.58% | Val_acc : 78.16% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 62.000 | Loss: 0.64061 | Val Loss: 0.64727 | Train acc: 78.65% | Val_acc : 78.25% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 63.000 | Loss: 0.63802 | Val Loss: 0.64477 | Train acc: 78.72% | Val_acc : 78.30% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 64.000 | Loss: 0.63548 | Val Loss: 0.64232 | Train acc: 78.80% | Val_acc : 78.40% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 65.000 | Loss: 0.63300 | Val Loss: 0.63993 | Train acc: 78.88% | Val_acc : 78.48% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 66.000 | Loss: 0.63058 | Val Loss: 0.63758 | Train acc: 78.93% | Val_acc : 78.52% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 67.000 | Loss: 0.62822 | Val Loss: 0.63529 | Train acc: 78.99% | Val_acc : 78.57% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 68.000 | Loss: 0.62590 | Val Loss: 0.63305 | Train acc: 79.08% | Val_acc : 78.64% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 69.000 | Loss: 0.62363 | Val Loss: 0.63086 | Train acc: 79.18% | Val_acc : 78.72% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 70.000 | Loss: 0.62141 | Val Loss: 0.62871 | Train acc: 79.26% | Val_acc : 78.79% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 71.000 | Loss: 0.61924 | Val Loss: 0.62661 | Train acc: 79.30% | Val_acc : 78.86% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 72.000 | Loss: 0.61711 | Val Loss: 0.62455 | Train acc: 79.37% | Val_acc : 78.94% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 73.000 | Loss: 0.61503 | Val Loss: 0.62253 | Train acc: 79.45% | Val_acc : 78.99% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 74.000 | Loss: 0.61299 | Val Loss: 0.62055 | Train acc: 79.53% | Val_acc : 79.07% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 75.000 | Loss: 0.61098 | Val Loss: 0.61861 | Train acc: 79.59% | Val_acc : 79.17% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 76.000 | Loss: 0.60902 | Val Loss: 0.61671 | Train acc: 79.65% | Val_acc : 79.22% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 77.000 | Loss: 0.60709 | Val Loss: 0.61484 | Train acc: 79.71% | Val_acc : 79.27% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 78.000 | Loss: 0.60520 | Val Loss: 0.61300 | Train acc: 79.77% | Val_acc : 79.33% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 79.000 | Loss: 0.60335 | Val Loss: 0.61120 | Train acc: 79.85% | Val_acc : 79.37% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 80.000 | Loss: 0.60152 | Val Loss: 0.60943 | Train acc: 79.90% | Val_acc : 79.43% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 81.000 | Loss: 0.59974 | Val Loss: 0.60769 | Train acc: 79.97% | Val_acc : 79.45% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 82.000 | Loss: 0.59798 | Val Loss: 0.60598 | Train acc: 80.01% | Val_acc : 79.50% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 83.000 | Loss: 0.59625 | Val Loss: 0.60430 | Train acc: 80.05% | Val_acc : 79.55% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 84.000 | Loss: 0.59455 | Val Loss: 0.60265 | Train acc: 80.08% | Val_acc : 79.59% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 85.000 | Loss: 0.59288 | Val Loss: 0.60104 | Train acc: 80.12% | Val_acc : 79.61% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 86.000 | Loss: 0.59124 | Val Loss: 0.59944 | Train acc: 80.16% | Val_acc : 79.63% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 87.000 | Loss: 0.58963 | Val Loss: 0.59787 | Train acc: 80.19% | Val_acc : 79.72% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 88.000 | Loss: 0.58804 | Val Loss: 0.59633 | Train acc: 80.23% | Val_acc : 79.72% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 89.000 | Loss: 0.58648 | Val Loss: 0.59481 | Train acc: 80.26% | Val_acc : 79.81% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 90.000 | Loss: 0.58495 | Val Loss: 0.59332 | Train acc: 80.30% | Val_acc : 79.86% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 91.000 | Loss: 0.58344 | Val Loss: 0.59185 | Train acc: 80.35% | Val_acc : 79.91% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 92.000 | Loss: 0.58195 | Val Loss: 0.59040 | Train acc: 80.38% | Val_acc : 79.97% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 93.000 | Loss: 0.58049 | Val Loss: 0.58898 | Train acc: 80.43% | Val_acc : 79.99% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 94.000 | Loss: 0.57905 | Val Loss: 0.58757 | Train acc: 80.48% | Val_acc : 80.07% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 95.000 | Loss: 0.57763 | Val Loss: 0.58619 | Train acc: 80.50% | Val_acc : 80.10% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 96.000 | Loss: 0.57623 | Val Loss: 0.58483 | Train acc: 80.57% | Val_acc : 80.13% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 97.000 | Loss: 0.57485 | Val Loss: 0.58349 | Train acc: 80.62% | Val_acc : 80.17% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 98.000 | Loss: 0.57350 | Val Loss: 0.58216 | Train acc: 80.65% | Val_acc : 80.23% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 99.000 | Loss: 0.57216 | Val Loss: 0.58086 | Train acc: 80.67% | Val_acc : 80.31% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 100.000 | Loss: 0.57084 | Val Loss: 0.57958 | Train acc: 80.72% | Val_acc : 80.32% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 101.000 | Loss: 0.56954 | Val Loss: 0.57831 | Train acc: 80.76% | Val_acc : 80.36% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 102.000 | Loss: 0.56826 | Val Loss: 0.57707 | Train acc: 80.79% | Val_acc : 80.38% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 103.000 | Loss: 0.56699 | Val Loss: 0.57584 | Train acc: 80.81% | Val_acc : 80.38% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 104.000 | Loss: 0.56574 | Val Loss: 0.57463 | Train acc: 80.83% | Val_acc : 80.41% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 105.000 | Loss: 0.56451 | Val Loss: 0.57343 | Train acc: 80.87% | Val_acc : 80.42% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 106.000 | Loss: 0.56330 | Val Loss: 0.57225 | Train acc: 80.90% | Val_acc : 80.47% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 107.000 | Loss: 0.56210 | Val Loss: 0.57109 | Train acc: 80.94% | Val_acc : 80.50% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 108.000 | Loss: 0.56092 | Val Loss: 0.56994 | Train acc: 80.97% | Val_acc : 80.54% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 109.000 | Loss: 0.55975 | Val Loss: 0.56880 | Train acc: 80.99% | Val_acc : 80.58% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 110.000 | Loss: 0.55860 | Val Loss: 0.56768 | Train acc: 81.03% | Val_acc : 80.62% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 111.000 | Loss: 0.55746 | Val Loss: 0.56658 | Train acc: 81.07% | Val_acc : 80.65% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 112.000 | Loss: 0.55634 | Val Loss: 0.56548 | Train acc: 81.10% | Val_acc : 80.67% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 113.000 | Loss: 0.55523 | Val Loss: 0.56441 | Train acc: 81.16% | Val_acc : 80.68% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 114.000 | Loss: 0.55413 | Val Loss: 0.56334 | Train acc: 81.20% | Val_acc : 80.70% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 115.000 | Loss: 0.55305 | Val Loss: 0.56229 | Train acc: 81.23% | Val_acc : 80.77% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 116.000 | Loss: 0.55198 | Val Loss: 0.56125 | Train acc: 81.25% | Val_acc : 80.79% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 117.000 | Loss: 0.55093 | Val Loss: 0.56022 | Train acc: 81.29% | Val_acc : 80.82% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 118.000 | Loss: 0.54988 | Val Loss: 0.55921 | Train acc: 81.32% | Val_acc : 80.87% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 119.000 | Loss: 0.54885 | Val Loss: 0.55821 | Train acc: 81.36% | Val_acc : 80.88% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 120.000 | Loss: 0.54783 | Val Loss: 0.55722 | Train acc: 81.38% | Val_acc : 80.92% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 121.000 | Loss: 0.54683 | Val Loss: 0.55624 | Train acc: 81.40% | Val_acc : 80.94% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 122.000 | Loss: 0.54583 | Val Loss: 0.55527 | Train acc: 81.44% | Val_acc : 80.97% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 123.000 | Loss: 0.54485 | Val Loss: 0.55431 | Train acc: 81.49% | Val_acc : 80.99% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 124.000 | Loss: 0.54388 | Val Loss: 0.55337 | Train acc: 81.52% | Val_acc : 81.01% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 125.000 | Loss: 0.54292 | Val Loss: 0.55243 | Train acc: 81.55% | Val_acc : 81.05% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 126.000 | Loss: 0.54197 | Val Loss: 0.55151 | Train acc: 81.57% | Val_acc : 81.04% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 127.000 | Loss: 0.54103 | Val Loss: 0.55060 | Train acc: 81.59% | Val_acc : 81.06% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 128.000 | Loss: 0.54010 | Val Loss: 0.54969 | Train acc: 81.61% | Val_acc : 81.08% |\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Epoch: 129.000 | Loss: 0.53918 | Val Loss: 0.54880 | Train acc: 81.63% | Val_acc : 81.10% |\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "seed = \"0100111001100101011101100110010101110010001000000110011101101111011011100110111001100001001000000110011101101001011101100110010100100000011110010110111101110101001000000111010101110000\"\n",
        "np.random.seed(abs(hash(seed))%2**32)\n",
        "\n",
        "\n",
        "\n",
        "fashion_mnist = NN(784, 190, 10, 0.12, 130)\n",
        "# No matter what , its saturating at around 80%. Tried scheduling lr , tried momentum, but nothing working.\n",
        "\n",
        "\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
        "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
        "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
        "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
        "\n",
        "# Training the model\n",
        "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
        "\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(train_loss,label='train', color = '#BADA55')\n",
        "plt.plot(val_loss,label='val', color = '#F00D00')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IGNORE\n",
        "\n",
        "\n",
        "# fashion_mnist = NN(784, 160, 10, 0.03, 130) # ---> Accuracy: 74.67 %   ----> Default\n",
        "# fashion_mnist = NN(784, 160, 10, 0.08, 130) # ---> Accuracy: 80.12 %\n",
        "# fashion_mnist = NN(784, 190, 10, 0.12, 130) # ---> Accuracy: 80.48 % --> train loss 0.618 at 59th epoch to 0.616 at the 60th epoch --> very slow learning so i am introducing momentum\n",
        "# fashion_mnist = NN(784, 190, 10, 0.01, 130, 1, 44, 0.9)   #----> introduced momentum  # Accuracy : 67.38%\n",
        "# fashion_mnist = NN(784, 190, 10, 0.1, 90, 1, 44, 0.9)   # Accuracy: 80.12 %\n",
        "# fashion_mnist = NN(784, 256, 10, 0.1, 90, 1, 44, 0.9)   # Accuracy: 79.57 %\n"
      ],
      "metadata": {
        "id": "Lue4VwMPkbQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9mWOuYl9MRue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f7fc1d-8ff9-4867-8a0b-a7356a57dbcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.42 %\n"
          ]
        }
      ],
      "source": [
        "y_pred = fashion_mnist.predict(X_test)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_test)*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val.shape"
      ],
      "metadata": {
        "id": "hbK7jokHphG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89fe016-f1e6-4726-b4e8-7d0e47927272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
